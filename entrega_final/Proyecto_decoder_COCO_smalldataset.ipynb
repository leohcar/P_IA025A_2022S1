{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto_decoder_with_dataset_COCO",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6a6896a69abe4551b67dbdcaa7282aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d3a31fec21fc4e81b8879d00aaf68aad",
              "IPY_MODEL_9a690b5d561f4b039bfe5bf351e98e2e",
              "IPY_MODEL_fceadad434164a91a477589e99a27bfd"
            ],
            "layout": "IPY_MODEL_a810b5d2773941d391299cab3eb65bc1"
          }
        },
        "d3a31fec21fc4e81b8879d00aaf68aad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7d15f30cd807467d91de6ee172debd64",
            "placeholder": "​",
            "style": "IPY_MODEL_47e576a6e27b45c0a01590435c0f0f26",
            "value": "Downloading: 100%"
          }
        },
        "9a690b5d561f4b039bfe5bf351e98e2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_88649422508f4768a8a02e828126e655",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_65b3b8e257174c6fb4e1c48f1b1672f3",
            "value": 29
          }
        },
        "fceadad434164a91a477589e99a27bfd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_20dfa5a152c6413987f3ae10cafd8c02",
            "placeholder": "​",
            "style": "IPY_MODEL_7b2fa408ed9c443ea212c47d151d6b93",
            "value": " 29.0/29.0 [00:00&lt;00:00, 1.02kB/s]"
          }
        },
        "a810b5d2773941d391299cab3eb65bc1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d15f30cd807467d91de6ee172debd64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "47e576a6e27b45c0a01590435c0f0f26": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "88649422508f4768a8a02e828126e655": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "65b3b8e257174c6fb4e1c48f1b1672f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20dfa5a152c6413987f3ae10cafd8c02": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b2fa408ed9c443ea212c47d151d6b93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4479a7c416354d7ead52e838069403b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d316264c062c4336a3a02ceb7ea3bf8c",
              "IPY_MODEL_2c9b2a047e874b5993ee7b4f33b02eee",
              "IPY_MODEL_43386727bdb14f20ae58f3ce30365664"
            ],
            "layout": "IPY_MODEL_d8cf219ac4fa496c9817fca85a25d941"
          }
        },
        "d316264c062c4336a3a02ceb7ea3bf8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_db191abe811d4ed6873d4f8717cbceb6",
            "placeholder": "​",
            "style": "IPY_MODEL_6af4595b45a54a3b855e063e9b64e1fb",
            "value": "Downloading: 100%"
          }
        },
        "2c9b2a047e874b5993ee7b4f33b02eee": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ae54076864054b69b851b7f372c23e2d",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_61744ac6f9b644cb8994a302f8bfdd64",
            "value": 213450
          }
        },
        "43386727bdb14f20ae58f3ce30365664": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5903952b8e2d4b36bec5ffdb080f4549",
            "placeholder": "​",
            "style": "IPY_MODEL_814e45d9a6e145b8b288b648fb41185b",
            "value": " 208k/208k [00:00&lt;00:00, 341kB/s]"
          }
        },
        "d8cf219ac4fa496c9817fca85a25d941": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "db191abe811d4ed6873d4f8717cbceb6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6af4595b45a54a3b855e063e9b64e1fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ae54076864054b69b851b7f372c23e2d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61744ac6f9b644cb8994a302f8bfdd64": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5903952b8e2d4b36bec5ffdb080f4549": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "814e45d9a6e145b8b288b648fb41185b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7c531b3700e4dae8d3cf62d7b7159c6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_962ec05cbd41480f8f061deae3901c65",
              "IPY_MODEL_ff07989c4ede469f8df3ebdddf446eff",
              "IPY_MODEL_a5588267a9c54fde88c5883ce0475aaf"
            ],
            "layout": "IPY_MODEL_8314955a521145ec98cc5b6c78473b40"
          }
        },
        "962ec05cbd41480f8f061deae3901c65": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f03858b8fe614d8383551d35bc1a0220",
            "placeholder": "​",
            "style": "IPY_MODEL_58fa8cfa4ecd483391a6ed3325931086",
            "value": "Downloading: 100%"
          }
        },
        "ff07989c4ede469f8df3ebdddf446eff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_10e587f4b58e4b62bbef9de75924d0ae",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_dd75806af7d849adbe2afed51f5ca46f",
            "value": 435797
          }
        },
        "a5588267a9c54fde88c5883ce0475aaf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_46a5253e7173474b8fe0d67684130938",
            "placeholder": "​",
            "style": "IPY_MODEL_63c600e5412d47ab9940252e14529214",
            "value": " 426k/426k [00:00&lt;00:00, 631kB/s]"
          }
        },
        "8314955a521145ec98cc5b6c78473b40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f03858b8fe614d8383551d35bc1a0220": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "58fa8cfa4ecd483391a6ed3325931086": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "10e587f4b58e4b62bbef9de75924d0ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dd75806af7d849adbe2afed51f5ca46f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "46a5253e7173474b8fe0d67684130938": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63c600e5412d47ab9940252e14529214": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e4721c2d837a42a1a3e7bbcd590f265a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_037d1e9553c94fc0a13eace64dd76258",
              "IPY_MODEL_8563152670754a538d63ddf131c3d052",
              "IPY_MODEL_2f09199135be4b9b978927d193b7a58e"
            ],
            "layout": "IPY_MODEL_e1f13044be344a0194223e64671fee8d"
          }
        },
        "037d1e9553c94fc0a13eace64dd76258": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3ba6269ef83c486d942353bf5818b533",
            "placeholder": "​",
            "style": "IPY_MODEL_7f374a77bb694d04a46b0e52f67047f4",
            "value": "Downloading: 100%"
          }
        },
        "8563152670754a538d63ddf131c3d052": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3f12027b55694bf6a3a01327d05f170d",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_e2f5f086d18c4e41816da1151a9bdb3f",
            "value": 570
          }
        },
        "2f09199135be4b9b978927d193b7a58e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dc4ab77f318b49f78730cda7393dd1fb",
            "placeholder": "​",
            "style": "IPY_MODEL_faea1abfb7c74fd981ed58e4dd518a6f",
            "value": " 570/570 [00:00&lt;00:00, 21.3kB/s]"
          }
        },
        "e1f13044be344a0194223e64671fee8d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3ba6269ef83c486d942353bf5818b533": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7f374a77bb694d04a46b0e52f67047f4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3f12027b55694bf6a3a01327d05f170d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e2f5f086d18c4e41816da1151a9bdb3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "dc4ab77f318b49f78730cda7393dd1fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "faea1abfb7c74fd981ed58e4dd518a6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohcar/P_IA025A_2022S1/blob/main/entrega_final/Proyecto_decoder_with_dataset_COCO_smalldataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Carlos Leonardo Ancasi Hinostroza'\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a927b60b-0756-4733-8757-16d42c9dc4c1"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Carlos Leonardo Ancasi Hinostroza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "c4ff9322-65a7-4d25-a041-eba6376b3e53"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 67.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 16.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI"
      ],
      "metadata": {
        "id": "4P-K3StTp76B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "50cd186e-f73c-4da7-81ac-585ba66306dd"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from pycocotools.coco import COCO"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbc8d8b8-1d0a-4733-d787-1f19c7488116"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 18 17:29:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   37C    P0    27W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35156b76-646f-43f8-fffe-76121eabfe71"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download BookCorpus"
      ],
      "metadata": {
        "id": "fHGebkCBqLPb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    # Recomenda-se usar o tokenizer.batch_encode_plus pois é mais rápido.\n",
        "    return (tokenizer.batch_encode_plus([text], return_tensors=None, add_special_tokens=False).input_ids)[0]\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        # Escreva aqui seu código.\n",
        "        self.x = []\n",
        "        self.max_seq_length = max_seq_length\n",
        "        x = [101]\n",
        "        x.extend([tokenizer.pad_token_id]*self.max_seq_length)        \n",
        "\n",
        "        for texto in texts:\n",
        "            token = tokenize(texto, tokenizer)\n",
        "            for i in range(0, len(token), (self.max_seq_length - 1) ):\n",
        "                context_size = (self.max_seq_length - 1)\n",
        "                if i +  max_seq_length - 1 > len(token):\n",
        "                    context_size = len(token) % (self.max_seq_length - 1)\n",
        "                x_a = x[:]\n",
        "                x_a[1:context_size+1]=token[i:i+context_size]\n",
        "\n",
        "                self.x.append(x_a)           \n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva aqui seu código.\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva aqui seu código.\n",
        "        return torch.LongTensor(self.x[idx][:-1]), torch.LongTensor(self.x[idx][1:])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=10)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "\n",
        "#assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "print(first_batch_input)\n",
        "print(first_batch_target)\n",
        "\n",
        "# assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "# assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "6a6896a69abe4551b67dbdcaa7282aa5",
            "d3a31fec21fc4e81b8879d00aaf68aad",
            "9a690b5d561f4b039bfe5bf351e98e2e",
            "fceadad434164a91a477589e99a27bfd",
            "a810b5d2773941d391299cab3eb65bc1",
            "7d15f30cd807467d91de6ee172debd64",
            "47e576a6e27b45c0a01590435c0f0f26",
            "88649422508f4768a8a02e828126e655",
            "65b3b8e257174c6fb4e1c48f1b1672f3",
            "20dfa5a152c6413987f3ae10cafd8c02",
            "7b2fa408ed9c443ea212c47d151d6b93",
            "4479a7c416354d7ead52e838069403b4",
            "d316264c062c4336a3a02ceb7ea3bf8c",
            "2c9b2a047e874b5993ee7b4f33b02eee",
            "43386727bdb14f20ae58f3ce30365664",
            "d8cf219ac4fa496c9817fca85a25d941",
            "db191abe811d4ed6873d4f8717cbceb6",
            "6af4595b45a54a3b855e063e9b64e1fb",
            "ae54076864054b69b851b7f372c23e2d",
            "61744ac6f9b644cb8994a302f8bfdd64",
            "5903952b8e2d4b36bec5ffdb080f4549",
            "814e45d9a6e145b8b288b648fb41185b",
            "c7c531b3700e4dae8d3cf62d7b7159c6",
            "962ec05cbd41480f8f061deae3901c65",
            "ff07989c4ede469f8df3ebdddf446eff",
            "a5588267a9c54fde88c5883ce0475aaf",
            "8314955a521145ec98cc5b6c78473b40",
            "f03858b8fe614d8383551d35bc1a0220",
            "58fa8cfa4ecd483391a6ed3325931086",
            "10e587f4b58e4b62bbef9de75924d0ae",
            "dd75806af7d849adbe2afed51f5ca46f",
            "46a5253e7173474b8fe0d67684130938",
            "63c600e5412d47ab9940252e14529214",
            "e4721c2d837a42a1a3e7bbcd590f265a",
            "037d1e9553c94fc0a13eace64dd76258",
            "8563152670754a538d63ddf131c3d052",
            "2f09199135be4b9b978927d193b7a58e",
            "e1f13044be344a0194223e64671fee8d",
            "3ba6269ef83c486d942353bf5818b533",
            "7f374a77bb694d04a46b0e52f67047f4",
            "3f12027b55694bf6a3a01327d05f170d",
            "e2f5f086d18c4e41816da1151a9bdb3f",
            "dc4ab77f318b49f78730cda7393dd1fb",
            "faea1abfb7c74fd981ed58e4dd518a6f"
          ]
        },
        "outputId": "2c87742d-6794-4631-ae8f-dfd4137caa25"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6a6896a69abe4551b67dbdcaa7282aa5"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4479a7c416354d7ead52e838069403b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7c531b3700e4dae8d3cf62d7b7159c6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e4721c2d837a42a1a3e7bbcd590f265a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de tamanho do dataset.\n",
            "tensor([[  101,   142,  1358,  1301, 12223,  1260,  1884, 26875,     0,     0],\n",
            "        [  101,  2896,  1161,  1301,  8419,   182, 10950,  1186,  1260,  1435],\n",
            "        [  101,  1197, 13473,     0,     0,     0,     0,     0,     0,     0]])\n",
            "tensor([[  142,  1358,  1301, 12223,  1260,  1884, 26875,     0,     0,     0],\n",
            "        [ 2896,  1161,  1301,  8419,   182, 10950,  1186,  1260,  1435,     0],\n",
            "        [ 1197, 13473,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "Passou no assert de dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset BookCorpus para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4luQpPPdQWNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f58b9f22-e006-46d3-fa4a-103c6892dd24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "max_seq_length = 12\n",
        "\n",
        "# data set pequeno\n",
        "train_examples = 1\n",
        "valid_examples = 200\n",
        "test_examples = 200\n",
        "\n",
        "path_text = F\"/content/drive/My Drive/modelo_proyecto/entrega_03/cocoSamples_01.txt\"\n",
        "texts = open(path_text).readlines()\n",
        "\n",
        "print(f'Read {len(texts)} lines.')\n",
        "\n",
        "max_lines = train_examples + valid_examples + test_examples\n",
        "print(f'Truncating to {max_lines} lines.') \n",
        "texts = texts[:max_lines]   \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cdcd262f-b2ef-4d0c-b921-66246db4dc92"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 120743 lines.\n",
            "Truncating to 401 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[150]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "_PMVVk3Z4VnI",
        "outputId": "9d1bae42-9bc3-47a4-e0ee-f62c89f0073f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'A woman cuts a large cake that has a lit candle.A woman with a knife cutting into a cake.a woman cutting into a big birthday cake with one candleA women cutting a large cake with one lit candle.A woman has her picture taken while she cuts a cake.\\n'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9323b325-b262-416a-88b6-cf1b72e2e8ab"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 6\n",
            "valid examples: 1204\n",
            "test examples: 1185\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(torch.nn.Module):\n",
        "    def __init__(self, dim, n_head):\n",
        "        \"\"\"\n",
        "        Implements the Multi head Attention\n",
        "\n",
        "        Args:\n",
        "            dim : Dimension of the embedding layer for each word in the context.\n",
        "            n_layers : number of self-attention layers.\n",
        "        \"\"\"\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        \n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "\n",
        "        # Dimension de cada head\n",
        "        self.dim_head = self.dim // self.n_head\n",
        "\n",
        "        # As matrixes W_k, W_q, W_v, W_e   \n",
        "        self.W_k = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_q = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_v = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_e = nn.Linear(self.dim, self.dim , bias=False)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, key, value, query, mask = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x is a LongTensor of shape (batch_size, max_seq_length, dimension)  (B,L,D)\n",
        "            mask is Tensor of shape (batch_size, 1, max_seq_length,max_seq_length) (B,1,L,L)\n",
        "        Returns:\n",
        "            LongTensor of shape (batch_size, max_seq_length, dimension)\n",
        "        \"\"\"\n",
        "        batch_size = value.size(0)\n",
        "        max_seq_length = value.size(1)\n",
        "\n",
        "        # k, q, v tem dimensão B, L, H, D/H \n",
        "        k = self.W_k(key).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "        q = self.W_q(query).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "        v = self.W_v(value).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "\n",
        "        # Transpor para B, H, L, D/H\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # atention\n",
        "        scores = torch.matmul(q, torch.transpose(k,-1,-2))   # B, H, L, L\n",
        "\n",
        "        # aplicar mascara\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(~mask, float(\"-1e16\"))\n",
        "\n",
        "        scores = scores/math.sqrt(self.dim_head)\n",
        "\n",
        "        # aplicar sofmax\n",
        "        probs = self.softmax(scores) # B, H, L, L\n",
        "\n",
        "        probs  = torch.matmul(probs, v) # B, H, L, D/H \n",
        "\n",
        "        probs = probs.transpose(1,2).contiguous() # B, L, H, D/H \n",
        "        probs = probs.reshape(batch_size,max_seq_length, self.dim) # B, L, D\n",
        "\n",
        "        return self.W_e(probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "a31ZiZEUdP9R"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "    def __init__(self, dim: int, n_head: int, expansion_factor: int):\n",
        "        \"\"\"\n",
        "        Implements Transformer Block \n",
        "        Args:\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_head (int): number of self-attention head\n",
        "        \"\"\"\n",
        "        super(TransformerBlock,self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        self.multi_head = MultiheadAttention(self.dim, self.n_head)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        hidden_size = self.expansion_factor * self.dim\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, self.dim)\n",
        "        )\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(self.dim)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, key, value, query, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x is a LongTensor of shape (batch_size, max_seq_length, dimension)  (B,L,D)\n",
        "            mask is Tensor of shape (batch_size, 1, max_seq_length,max_seq_length) (B,1,L,L)\n",
        "        Returns:\n",
        "            LongTensor of shape (batch_size, max_seq_length, dimension)\n",
        "        \"\"\"\n",
        "\n",
        "        attention = self.multi_head(key, value, query, mask=mask)\n",
        "        attention_residual = attention + query\n",
        "        norm1_out = self.dropout1(self.norm1(attention_residual))\n",
        "        \n",
        "        feed_fwd = self.feed_forward(norm1_out)\n",
        "        feed_fwd_residual = feed_fwd + norm1_out\n",
        "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual))\n",
        "        \n",
        "        return norm2_out\n"
      ],
      "metadata": {
        "id": "FA0PkKhVwjjA"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(torch.nn.Module):\n",
        "    def __init__(self, dim: int, n_head: int, expansion_factor: int):\n",
        "\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "        self.expasion_factor = expansion_factor\n",
        "\n",
        "        self.attention = MultiheadAttention(self.dim, self.n_head)\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        self.transformer_block = TransformerBlock(self.dim, self.n_head, self.expasion_factor)\n",
        "    \n",
        "    def forward(self, enc_out, x, mask_self):\n",
        "\n",
        "        attention = self.attention(x, x, x, mask_self)\n",
        "        attention_residual = attention + x\n",
        "        out = self.dropout1(self.norm1(attention_residual))\n",
        "\n",
        "        if enc_out is None:\n",
        "            enc_out = out\n",
        "        out = self.transformer_block(enc_out,enc_out,out)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "HCXmrfIQrYu4"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "class TransformerDecoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int, n_head: int, expansion_factor: int):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "            n_head(int): number of head of self-attention\n",
        "            expansion_factor(int): fator for the hidden size of feed_forward\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super(TransformerDecoder,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.dim = dim\n",
        "        self.n_layers = n_layers\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        # C()\n",
        "        self.C_w = nn.Embedding(vocab_size, dim)\n",
        "\n",
        "        # P()\n",
        "        self.P_w = nn.Embedding(max_seq_length, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderBlock(self.dim,self.n_head,self.expansion_factor) for i in range(self.n_layers)])\n",
        "\n",
        "        self.linear_out = nn.Linear(self.dim,self.vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, enc_out=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            \n",
        "        Returns:\n",
        "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        \n",
        "        batch_size = inputs.size(0)\n",
        "        max_seq_length = inputs.size(1)\n",
        "\n",
        "        c_emb = self.C_w(inputs)  # B,L,D\n",
        "        p_emb = self.P_w(torch.LongTensor(range(0,self.max_seq_length)).unsqueeze(0).to(inputs.device))\n",
        "\n",
        "\n",
        "        x = c_emb + p_emb  # B,L,D\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # generar mascara\n",
        "        mask_tri = (torch.ones(self.max_seq_length,self.max_seq_length).tril() == 1).expand(batch_size,1,self.max_seq_length,self.max_seq_length).to(inputs.device)\n",
        "        \n",
        "        mask_pad = (inputs != self.pad_token_id)\n",
        "        mask_pad = mask_pad.reshape(batch_size,1,1,self.max_seq_length).expand(batch_size,1,self.max_seq_length,self.max_seq_length).to(inputs.device)\n",
        "\n",
        "        mask = torch.logical_and(mask_tri,mask_pad)\n",
        "\n",
        "        for layer in self.layers:\n",
        "\n",
        "            x = layer(enc_out,x,mask_self=mask)   # B, L, D\n",
        "\n",
        "        out = self.linear_out(x) # B, L, vocab\n",
        "\n",
        "        return out"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51e5d652-732f-4413-b8de-2b8a044b9d95"
      },
      "source": [
        "model = TransformerDecoder(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=256,\n",
        "    n_layers=4,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    n_head = 4,\n",
        "    expansion_factor = 4,\n",
        ").to(device)\n",
        "\n",
        "sample_input, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_input.to(device)\n",
        "sample_output = model(sample_input)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_input.shape: torch.Size([1, 12])\n",
            "sample_output.shape: torch.Size([1, 12, 28996])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86575eee-ca96-48f5-dc3b-9fd3cec4b98d"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 19083588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model(train_input_ids)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6db027e-748a-466b-adac-f126d222f5ad"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              35268\n",
            "correct initial perplexity: 28996\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'model_decoder_smalldataset.pt'\n",
        "path_modelo = F\"/content/drive/My Drive/modelo_proyecto/entrega_03/pre_trained/{model_save_name}\""
      ],
      "metadata": {
        "id": "lyb-He4AFHFP"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bbc3b593-8fb5-4aee-f03e-8b6b25c8d601"
      },
      "source": [
        "max_examples = 150_000_000\n",
        "eval_every_steps = 100\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "dim = 256\n",
        "n_layers = 4\n",
        "n_head = 4\n",
        "expansion_factor = 4\n",
        "\n",
        "\n",
        "model = TransformerDecoder(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=dim,\n",
        "    n_layers=n_layers,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    n_head = n_head,\n",
        "    expansion_factor = expansion_factor,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=6, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=64)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "checkpoint = {'vocab_size': tokenizer.vocab_size,\n",
        "              'max_seq_length': max_seq_length,\n",
        "              'dim': dim,\n",
        "              'n_layers': n_layers,\n",
        "              'pad_token_id': tokenizer.pad_token_id,\n",
        "              'n_head': n_head,\n",
        "              'expansion_factor': expansion_factor,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict' : optimizer.state_dict(),\n",
        "              'step': [],\n",
        "              'n_examples': [],\n",
        "              'train_ppl': [],\n",
        "              'valid_ppl': []\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, path_modelo)\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_loss = np.average(train_losses)\n",
        "            train_ppl = np.exp(train_loss)\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.6f}, train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "            \n",
        "            checkpoint['step'].append(n_examples)\n",
        "            checkpoint['n_examples'].append(n_examples)\n",
        "            checkpoint['train_ppl'].append(train_ppl)\n",
        "            checkpoint['valid_ppl'].append(valid_ppl)\n",
        "\n",
        "            checkpoint['model_state_dict'] = model.state_dict()\n",
        "            checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "            torch.save(checkpoint, path_modelo)\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train loss: 10.498112, train ppl: 36246.99, valid ppl: 31262.70\n",
            "100 steps; 600 examples so far; train loss: 3.026780, train ppl: 20.63, valid ppl: 32645.66\n",
            "200 steps; 1200 examples so far; train loss: 0.074505, train ppl: 1.08, valid ppl: 61378.85\n",
            "300 steps; 1800 examples so far; train loss: 0.025806, train ppl: 1.03, valid ppl: 87634.02\n",
            "400 steps; 2400 examples so far; train loss: 0.014585, train ppl: 1.01, valid ppl: 114336.98\n",
            "500 steps; 3000 examples so far; train loss: 0.009695, train ppl: 1.01, valid ppl: 139948.92\n",
            "600 steps; 3600 examples so far; train loss: 0.007083, train ppl: 1.01, valid ppl: 167988.77\n",
            "700 steps; 4200 examples so far; train loss: 0.005384, train ppl: 1.01, valid ppl: 195241.52\n",
            "800 steps; 4800 examples so far; train loss: 0.004191, train ppl: 1.00, valid ppl: 220418.95\n",
            "900 steps; 5400 examples so far; train loss: 0.003417, train ppl: 1.00, valid ppl: 250036.50\n",
            "1000 steps; 6000 examples so far; train loss: 0.002862, train ppl: 1.00, valid ppl: 277906.81\n",
            "1100 steps; 6600 examples so far; train loss: 0.002411, train ppl: 1.00, valid ppl: 312126.77\n",
            "1200 steps; 7200 examples so far; train loss: 0.002066, train ppl: 1.00, valid ppl: 342107.42\n",
            "1300 steps; 7800 examples so far; train loss: 0.001749, train ppl: 1.00, valid ppl: 369078.78\n",
            "1400 steps; 8400 examples so far; train loss: 0.001557, train ppl: 1.00, valid ppl: 406017.17\n",
            "1500 steps; 9000 examples so far; train loss: 0.001369, train ppl: 1.00, valid ppl: 438754.78\n",
            "1600 steps; 9600 examples so far; train loss: 0.001213, train ppl: 1.00, valid ppl: 471941.06\n",
            "1700 steps; 10200 examples so far; train loss: 0.001085, train ppl: 1.00, valid ppl: 510244.57\n",
            "1800 steps; 10800 examples so far; train loss: 0.000971, train ppl: 1.00, valid ppl: 541633.09\n",
            "1900 steps; 11400 examples so far; train loss: 0.000891, train ppl: 1.00, valid ppl: 582989.98\n",
            "2000 steps; 12000 examples so far; train loss: 0.000787, train ppl: 1.00, valid ppl: 614966.16\n",
            "2100 steps; 12600 examples so far; train loss: 0.000716, train ppl: 1.00, valid ppl: 659067.71\n",
            "2200 steps; 13200 examples so far; train loss: 0.000659, train ppl: 1.00, valid ppl: 700698.59\n",
            "2300 steps; 13800 examples so far; train loss: 0.000591, train ppl: 1.00, valid ppl: 739128.16\n",
            "2400 steps; 14400 examples so far; train loss: 0.000547, train ppl: 1.00, valid ppl: 781473.37\n",
            "2500 steps; 15000 examples so far; train loss: 0.000509, train ppl: 1.00, valid ppl: 828206.06\n",
            "2600 steps; 15600 examples so far; train loss: 0.000464, train ppl: 1.00, valid ppl: 868490.70\n",
            "2700 steps; 16200 examples so far; train loss: 0.000429, train ppl: 1.00, valid ppl: 918836.63\n",
            "2800 steps; 16800 examples so far; train loss: 0.000399, train ppl: 1.00, valid ppl: 964003.45\n",
            "2900 steps; 17400 examples so far; train loss: 0.000368, train ppl: 1.00, valid ppl: 1016570.25\n",
            "3000 steps; 18000 examples so far; train loss: 0.000345, train ppl: 1.00, valid ppl: 1075200.06\n",
            "3100 steps; 18600 examples so far; train loss: 0.000320, train ppl: 1.00, valid ppl: 1127120.43\n",
            "3200 steps; 19200 examples so far; train loss: 0.000291, train ppl: 1.00, valid ppl: 1177793.62\n",
            "3300 steps; 19800 examples so far; train loss: 0.000274, train ppl: 1.00, valid ppl: 1231724.26\n",
            "3400 steps; 20400 examples so far; train loss: 0.000257, train ppl: 1.00, valid ppl: 1296448.68\n",
            "3500 steps; 21000 examples so far; train loss: 0.000237, train ppl: 1.00, valid ppl: 1357719.29\n",
            "3600 steps; 21600 examples so far; train loss: 0.000221, train ppl: 1.00, valid ppl: 1416836.41\n",
            "3700 steps; 22200 examples so far; train loss: 0.000207, train ppl: 1.00, valid ppl: 1488040.76\n",
            "3800 steps; 22800 examples so far; train loss: 0.000195, train ppl: 1.00, valid ppl: 1567273.05\n",
            "3900 steps; 23400 examples so far; train loss: 0.000185, train ppl: 1.00, valid ppl: 1630625.97\n",
            "4000 steps; 24000 examples so far; train loss: 0.000169, train ppl: 1.00, valid ppl: 1702046.72\n",
            "4100 steps; 24600 examples so far; train loss: 0.000161, train ppl: 1.00, valid ppl: 1768035.92\n",
            "4200 steps; 25200 examples so far; train loss: 0.000152, train ppl: 1.00, valid ppl: 1848172.24\n",
            "4300 steps; 25800 examples so far; train loss: 0.000142, train ppl: 1.00, valid ppl: 1910252.46\n",
            "4400 steps; 26400 examples so far; train loss: 0.000134, train ppl: 1.00, valid ppl: 2004615.31\n",
            "4500 steps; 27000 examples so far; train loss: 0.000125, train ppl: 1.00, valid ppl: 2087450.65\n",
            "4600 steps; 27600 examples so far; train loss: 0.000118, train ppl: 1.00, valid ppl: 2185409.38\n",
            "4700 steps; 28200 examples so far; train loss: 0.000111, train ppl: 1.00, valid ppl: 2282638.43\n",
            "4800 steps; 28800 examples so far; train loss: 0.000106, train ppl: 1.00, valid ppl: 2384649.79\n",
            "4900 steps; 29400 examples so far; train loss: 0.000098, train ppl: 1.00, valid ppl: 2468568.63\n",
            "5000 steps; 30000 examples so far; train loss: 0.000093, train ppl: 1.00, valid ppl: 2578847.81\n",
            "5100 steps; 30600 examples so far; train loss: 0.000087, train ppl: 1.00, valid ppl: 2666230.83\n",
            "5200 steps; 31200 examples so far; train loss: 0.000084, train ppl: 1.00, valid ppl: 2790709.33\n",
            "5300 steps; 31800 examples so far; train loss: 0.000078, train ppl: 1.00, valid ppl: 2902534.51\n",
            "5400 steps; 32400 examples so far; train loss: 0.000073, train ppl: 1.00, valid ppl: 3020994.69\n",
            "5500 steps; 33000 examples so far; train loss: 0.000070, train ppl: 1.00, valid ppl: 3161679.30\n",
            "5600 steps; 33600 examples so far; train loss: 0.000065, train ppl: 1.00, valid ppl: 3286057.35\n",
            "5700 steps; 34200 examples so far; train loss: 0.000062, train ppl: 1.00, valid ppl: 3413443.68\n",
            "5800 steps; 34800 examples so far; train loss: 0.000059, train ppl: 1.00, valid ppl: 3550470.21\n",
            "5900 steps; 35400 examples so far; train loss: 0.000055, train ppl: 1.00, valid ppl: 3683446.30\n",
            "6000 steps; 36000 examples so far; train loss: 0.000052, train ppl: 1.00, valid ppl: 3843810.14\n",
            "6100 steps; 36600 examples so far; train loss: 0.000050, train ppl: 1.00, valid ppl: 3968279.12\n",
            "6200 steps; 37200 examples so far; train loss: 0.000047, train ppl: 1.00, valid ppl: 4130659.98\n",
            "6300 steps; 37800 examples so far; train loss: 0.000044, train ppl: 1.00, valid ppl: 4274774.81\n",
            "6400 steps; 38400 examples so far; train loss: 0.000042, train ppl: 1.00, valid ppl: 4450079.32\n",
            "6500 steps; 39000 examples so far; train loss: 0.000040, train ppl: 1.00, valid ppl: 4648489.28\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 379\u001b[0;31m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    380\u001b[0m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_save\u001b[0;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[1;32m    603\u001b[0m         \u001b[0mnum_bytes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 604\u001b[0;31m         \u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstorage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_ptr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    605\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-622379c538a8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     92\u001b[0m             \u001b[0mcheckpoint\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'optimizer_state_dict'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheckpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath_modelo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mn_examples\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Increment of batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[1;32m    378\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0m_open_zipfile_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopened_file\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m                 \u001b[0m_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 380\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    381\u001b[0m         \u001b[0m_legacy_save\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_module\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpickle_protocol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 259\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite_end_of_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    260\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: [enforce fail at inline_container.cc:319] . unexpected pos 196742400 vs 196742288"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "outputId": "a4dd00cb-668a-4231-c480-8a4037311a3f",
        "id": "pGLLmAWybn9K"
      },
      "source": [
        "# seguir entrenando modelo guardado\n",
        "max_examples = 150_000_000\n",
        "eval_every_steps = 10000\n",
        "lr = 3e-4\n",
        "\n",
        "checkpoint = torch.load(path_modelo)\n",
        "model = TransformerDecoder(\n",
        "    vocab_size = checkpoint['vocab_size'],\n",
        "    max_seq_length = checkpoint['max_seq_length'],\n",
        "    dim = checkpoint['dim'],\n",
        "    n_layers = checkpoint['n_layers'],\n",
        "    pad_token_id = checkpoint['pad_token_id'],\n",
        "    n_head = checkpoint['n_head'],\n",
        "    expansion_factor = checkpoint['expansion_factor'],\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=64)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for s in zip(checkpoint['step'], checkpoint['n_examples'], checkpoint['train_ppl'], checkpoint['valid_ppl']):\n",
        "    print(f'{s[0]} steps; {s[1]} examples so far; train ppl: {s[2]:.2f}, valid ppl: {s[3]:.2f}')\n",
        "\n",
        "n_examples = checkpoint['n_examples'][-1]\n",
        "step = checkpoint['step'][-1]\n",
        "\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_ppl = np.exp(np.average(train_losses))\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "            \n",
        "            checkpoint['step'].append(n_examples)\n",
        "            checkpoint['n_examples'].append(n_examples)\n",
        "            checkpoint['train_ppl'].append(train_ppl)\n",
        "            checkpoint['valid_ppl'].append(valid_ppl)\n",
        "\n",
        "            checkpoint['model_state_dict'] = model.state_dict()\n",
        "            checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "            torch.save(checkpoint, path_modelo)\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 33457.80, valid ppl: 23545.12\n",
            "640000 steps; 640000 examples so far; train ppl: 281.77, valid ppl: 236.29\n",
            "1280000 steps; 1280000 examples so far; train ppl: 182.81, valid ppl: 209.93\n",
            "1920000 steps; 1920000 examples so far; train ppl: 162.02, valid ppl: 195.25\n",
            "1920000 steps; 1920000 examples so far; train ppl: 169.67, valid ppl: 195.38\n",
            "2560000 steps; 2560000 examples so far; train ppl: 150.07, valid ppl: 188.94\n",
            "3200000 steps; 3200000 examples so far; train ppl: 143.30, valid ppl: 181.40\n",
            "3200000 steps; 3200000 examples so far; train ppl: 143.26, valid ppl: 181.26\n",
            "3840000 steps; 3840000 examples so far; train ppl: 137.85, valid ppl: 177.87\n",
            "4480000 steps; 4480000 examples so far; train ppl: 134.42, valid ppl: 172.86\n",
            "4480000 steps; 4480000 examples so far; train ppl: 156.68, valid ppl: 173.00\n",
            "4490000 steps; 5120000 examples so far; train ppl: 130.95, valid ppl: 170.85\n",
            "4500000 steps; 5760000 examples so far; train ppl: 128.62, valid ppl: 168.10\n",
            "4510000 steps; 6400000 examples so far; train ppl: 127.42, valid ppl: 167.16\n",
            "4520000 steps; 7040000 examples so far; train ppl: 125.70, valid ppl: 164.40\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-65-21a18c5461be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mn_examples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-65-21a18c5461be>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(input_ids, target_ids)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/adam.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    151\u001b[0m                    \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'weight_decay'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m                    \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'eps'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m                    maximize=group['maximize'])\n\u001b[0m\u001b[1;32m    154\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/_functional.py\u001b[0m in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    108\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m def adamw(params: List[Tensor],\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c742d54-5e01-467b-dc13-8106d9a6c76f"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 158.41590589024548\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A man is eating pizza in the park at night with a'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e15be28f-0ce4-4519-b009-f1deea459117"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man is eating pizza in the park at night with aering\n",
            "A man is eating pizza in the park at night with aering few\n",
            "A man is eating pizza in the park at night with aering few -\n",
            "A man is eating pizza in the park at night with aering few - minutes\n",
            "A man is eating pizza in the park at night with aering few - minutes foot\n",
            "A man is eating pizza in the park at night with aering few - minutes foot.\n",
            "A man is eating pizza in the park at night with aering few - minutes foot.falls\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls.\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don of\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don oft\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don oft the\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don oft the the\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don oft the the man\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don oft the the manrians\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don oft the the manrians.\n",
            "A man is eating pizza in the park at night with aering few - minutes foot. falls. the i first don oft the the manrians..\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The young dog is sleeping in the park and playing with an'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f666702-a5c1-402d-8732-60e180054819",
        "id": "EwHpxq6zltEC"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The young dog is sleeping in the park and playing with an a\n",
            "The young dog is sleeping in the park and playing with an a old\n",
            "The young dog is sleeping in the park and playing with an a old few\n",
            "The young dog is sleeping in the park and playing with an a old few man\n",
            "The young dog is sleeping in the park and playing with an a old few man of\n",
            "The young dog is sleeping in the park and playing with an a old few man of with\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a other\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf side\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica of\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica ofn\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica ofn the\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica ofn the the\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica ofn the the first\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica ofn the the first man\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica ofn the the first man time\n",
            "The young dog is sleeping in the park and playing with an a old few man of with the a otherf siderica ofn the the first man time who\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The old dog is running in the park and playing with a'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    \n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY8iKefm8IiU",
        "outputId": "c64dffaa-6ce4-4be4-ffdb-a31f387cfce0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The old dog is running in the park and playing with a the\n",
            "The old dog is running in the park and playing with a the lot\n",
            "The old dog is running in the park and playing with a the lot door\n",
            "The old dog is running in the park and playing with a the lot door of\n",
            "The old dog is running in the park and playing with a the lot door of.\n",
            "The old dog is running in the park and playing with a the lot door of. the\n",
            "The old dog is running in the park and playing with a the lot door of. the the\n",
            "The old dog is running in the park and playing with a the lot door of. the the man\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was,\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a a\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a a man\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a a manpot\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a a manpot,\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a a manpot,hos\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a a manpot, hos,\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a a manpot, hos,,\n",
            "The old dog is running in the park and playing with a the lot door of. the the man first was, a a manpot, hos,, and\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}
