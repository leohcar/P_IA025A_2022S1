{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Proyecto_decoder_with_dataset_bookcorpus",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard",
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "28c4996c608f41abb0fb829b934f25be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fc7ad48df86341a49afec6012845cc74",
              "IPY_MODEL_55eee8c47ffc4b18a17ae6901695d222",
              "IPY_MODEL_d7929a6719bd4c2589aa38a3190d75c0"
            ],
            "layout": "IPY_MODEL_dcba05f54c9c4d4b8a4abe92a7dcf7bd"
          }
        },
        "fc7ad48df86341a49afec6012845cc74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f322c67311434e4999c53b981f40ca6e",
            "placeholder": "​",
            "style": "IPY_MODEL_3bdeef6fbcfd40a085ae4b7758a03a4b",
            "value": "Downloading: 100%"
          }
        },
        "55eee8c47ffc4b18a17ae6901695d222": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_492ca6f0c3d545778dd9d8506370dfae",
            "max": 29,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d39f2e06e98047e394d3cf60938276c4",
            "value": 29
          }
        },
        "d7929a6719bd4c2589aa38a3190d75c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_825e14cb5abf4076ba6b8b7cc7c337d0",
            "placeholder": "​",
            "style": "IPY_MODEL_6ba54458906c454a9a08f09181a89764",
            "value": " 29.0/29.0 [00:00&lt;00:00, 808B/s]"
          }
        },
        "dcba05f54c9c4d4b8a4abe92a7dcf7bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f322c67311434e4999c53b981f40ca6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3bdeef6fbcfd40a085ae4b7758a03a4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "492ca6f0c3d545778dd9d8506370dfae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d39f2e06e98047e394d3cf60938276c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "825e14cb5abf4076ba6b8b7cc7c337d0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ba54458906c454a9a08f09181a89764": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "298f051fb63f4c7c90825285c7ed9831": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e2d8b5da8c3d48d495bfa23b45f7d99a",
              "IPY_MODEL_3993ae92f1fc40f7bb1c115db62d0d7e",
              "IPY_MODEL_5aae4244e83344fdbe54339d2384abf0"
            ],
            "layout": "IPY_MODEL_da906a3b87da4d868877f529de6de6ea"
          }
        },
        "e2d8b5da8c3d48d495bfa23b45f7d99a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de6b503848084cf9944689b2cfc8d530",
            "placeholder": "​",
            "style": "IPY_MODEL_30407a36393e44c89a0c3b0d160b40e4",
            "value": "Downloading: 100%"
          }
        },
        "3993ae92f1fc40f7bb1c115db62d0d7e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_392a6b47f8434ee3a579c79d271d343f",
            "max": 213450,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_550c7db29e64489fb4f12358268db584",
            "value": 213450
          }
        },
        "5aae4244e83344fdbe54339d2384abf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bb12cee440ac4ec387590e2c1d22b5f6",
            "placeholder": "​",
            "style": "IPY_MODEL_b0a992b87af244809bf0068cb45c26e2",
            "value": " 208k/208k [00:00&lt;00:00, 211kB/s]"
          }
        },
        "da906a3b87da4d868877f529de6de6ea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de6b503848084cf9944689b2cfc8d530": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30407a36393e44c89a0c3b0d160b40e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "392a6b47f8434ee3a579c79d271d343f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "550c7db29e64489fb4f12358268db584": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb12cee440ac4ec387590e2c1d22b5f6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b0a992b87af244809bf0068cb45c26e2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c9358d659a3a438aac735d3ba30b26de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_be3767758d55484daccb476c32e0effd",
              "IPY_MODEL_3ef8d909ad66476e8929ec101bc0052a",
              "IPY_MODEL_75b6ebef958541aa84950171d325e8ea"
            ],
            "layout": "IPY_MODEL_a6983efda783484dacb2db6d81f0740b"
          }
        },
        "be3767758d55484daccb476c32e0effd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2bac18e142cc4dd598b93b583c81ee90",
            "placeholder": "​",
            "style": "IPY_MODEL_7a7e1ac52e284325bcb181306d18344b",
            "value": "Downloading: 100%"
          }
        },
        "3ef8d909ad66476e8929ec101bc0052a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9eac330714f47898af6ce47a2a38f76",
            "max": 435797,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_382f2a14658c49eb8ae9b6ac0b13c2f1",
            "value": 435797
          }
        },
        "75b6ebef958541aa84950171d325e8ea": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d959a3b07764c0792372268dfddb420",
            "placeholder": "​",
            "style": "IPY_MODEL_3792284e58b24e55b44b211c679fe124",
            "value": " 426k/426k [00:00&lt;00:00, 383kB/s]"
          }
        },
        "a6983efda783484dacb2db6d81f0740b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2bac18e142cc4dd598b93b583c81ee90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a7e1ac52e284325bcb181306d18344b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9eac330714f47898af6ce47a2a38f76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "382f2a14658c49eb8ae9b6ac0b13c2f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4d959a3b07764c0792372268dfddb420": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3792284e58b24e55b44b211c679fe124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc29c3667bd447059c9dc4739ee64b9c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_27fea7f0ada5442bbfc956b69ad342f0",
              "IPY_MODEL_6c6dd2760a4d4baab493f302d4e455cc",
              "IPY_MODEL_3809c9a60a5a4be488069186acb7f045"
            ],
            "layout": "IPY_MODEL_fce39bf847064ca685cd029aabd9308f"
          }
        },
        "27fea7f0ada5442bbfc956b69ad342f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c22932a02ad4465f89e5618e9f8ad1f4",
            "placeholder": "​",
            "style": "IPY_MODEL_c7fb62bda8794fd9b7008eba755bdf09",
            "value": "Downloading: 100%"
          }
        },
        "6c6dd2760a4d4baab493f302d4e455cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f641a39f115404fad5a9d922cb1a33a",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27dd862566754a2d9eb1429f5028deb4",
            "value": 570
          }
        },
        "3809c9a60a5a4be488069186acb7f045": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_820cfdc5a1a44c03b4c0ed70bc4fcbbb",
            "placeholder": "​",
            "style": "IPY_MODEL_dc3a24f07f1240449efcc5e3b2a397f0",
            "value": " 570/570 [00:00&lt;00:00, 17.2kB/s]"
          }
        },
        "fce39bf847064ca685cd029aabd9308f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c22932a02ad4465f89e5618e9f8ad1f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c7fb62bda8794fd9b7008eba755bdf09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7f641a39f115404fad5a9d922cb1a33a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27dd862566754a2d9eb1429f5028deb4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "820cfdc5a1a44c03b4c0ed70bc4fcbbb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "dc3a24f07f1240449efcc5e3b2a397f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/leohcar/P_IA025A_2022S1/blob/main/entrega_final/Proyecto_decoder_bookcorpus.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nome = 'Carlos Leonardo Ancasi Hinostroza'\n",
        "print(f'Meu nome é {nome}')"
      ],
      "metadata": {
        "id": "jOdQB41_4ZxG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47525585-8a2d-45ea-d26b-9feef530edf7"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Meu nome é Carlos Leonardo Ancasi Hinostroza\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2IbuChoAPMEn"
      },
      "source": [
        "#  Decoder"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# iremos utilizar a biblioteca dos transformers para ter acesso ao tokenizador do BERT.\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3twP0YJC4jmJ",
        "outputId": "841f2a8b-1d86-4739-886b-cab1b098d723"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.20.1-py3-none-any.whl (4.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.4 MB 4.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.7.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 33.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.21.6)\n",
            "Collecting pyyaml>=5.1\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 55.6 MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.64.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Collecting huggingface-hub<1.0,>=0.1.0\n",
            "  Downloading huggingface_hub-0.8.1-py3-none-any.whl (101 kB)\n",
            "\u001b[K     |████████████████████████████████| 101 kB 10.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.8.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Installing collected packages: pyyaml, tokenizers, huggingface-hub, transformers\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "Successfully installed huggingface-hub-0.8.1 pyyaml-6.0 tokenizers-0.12.1 transformers-4.20.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q git+https://github.com/philferriere/cocoapi.git#subdirectory=PythonAPI"
      ],
      "metadata": {
        "id": "4P-K3StTp76B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "648df049-18e0-457d-8fc5-1bdee9d9ebdb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Building wheel for pycocotools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YnyhJZtTRNMx"
      },
      "source": [
        "## Importação dos pacotes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlIOVCajPWcU"
      },
      "source": [
        "import collections\n",
        "import itertools\n",
        "import functools\n",
        "import math\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm_notebook\n",
        "\n",
        "from pycocotools.coco import COCO"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check which GPU we are using\n",
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "w9f3PfifAwpU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0baed5a6-2ec7-477b-ae5d-04824a7d5658"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul 18 19:42:55 2022       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P0    28W / 250W |      2MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available(): \n",
        "   dev = \"cuda:0\"\n",
        "else: \n",
        "   dev = \"cpu\"\n",
        "device = torch.device(dev)\n",
        "print('Using {}'.format(device))"
      ],
      "metadata": {
        "id": "whTCe2i7AtoV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "165e0826-5950-4640-fdec-5a8d42430e51"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download BookCorpus"
      ],
      "metadata": {
        "id": "fHGebkCBqLPb"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZfxgV2DUk58"
      },
      "source": [
        "## Implementação do MyDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n_xhKm1EZ3bQ"
      },
      "source": [
        "from typing import List\n",
        "\n",
        "def tokenize(text: str, tokenizer):\n",
        "    # Recomenda-se usar o tokenizer.batch_encode_plus pois é mais rápido.\n",
        "    return (tokenizer.batch_encode_plus([text], return_tensors=None, add_special_tokens=False).input_ids)[0]\n",
        "\n",
        "\n",
        "class MyDataset():\n",
        "    def __init__(self, texts: List[str], tokenizer, max_seq_length: int):\n",
        "        # Escreva aqui seu código.\n",
        "        self.x = []\n",
        "        self.max_seq_length = max_seq_length\n",
        "        x = [101]\n",
        "        x.extend([tokenizer.pad_token_id]*self.max_seq_length)        \n",
        "\n",
        "        for texto in texts:\n",
        "            token = tokenize(texto, tokenizer)\n",
        "            for i in range(0, len(token), (self.max_seq_length - 1) ):\n",
        "                context_size = (self.max_seq_length - 1)\n",
        "                if i +  max_seq_length - 1 > len(token):\n",
        "                    context_size = len(token) % (self.max_seq_length - 1)\n",
        "                x_a = x[:]\n",
        "                x_a[1:context_size+1]=token[i:i+context_size]\n",
        "\n",
        "                self.x.append(x_a)           \n",
        "\n",
        "    def __len__(self):\n",
        "        # Escreva aqui seu código.\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Escreva aqui seu código.\n",
        "        return torch.LongTensor(self.x[idx][:-1]), torch.LongTensor(self.x[idx][1:])"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Testando se a implementação do MyDataset está correta"
      ],
      "metadata": {
        "id": "wew-gFbWeBTq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import BertTokenizerFast\n",
        "\n",
        "tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "dummy_texts = ['Eu gosto de correr', 'Ela gosta muito de comer pizza']\n",
        "\n",
        "dummy_dataset = MyDataset(texts=dummy_texts, tokenizer=tokenizer, max_seq_length=10)\n",
        "dummy_loader = DataLoader(dummy_dataset, batch_size=6, shuffle=False)\n",
        "\n",
        "#assert len(dummy_dataset) == 2\n",
        "print('Passou no assert de tamanho do dataset.')\n",
        "\n",
        "first_batch_input, first_batch_target = next(iter(dummy_loader))\n",
        "correct_first_batch_input = torch.LongTensor(\n",
        "    [[  101,  3396, 10303,   125, 13239,     0,     0,     0,     0],\n",
        "     [  101,  1660,  5971,   785,   125,  1847, 13779, 15616,     0]])\n",
        "\n",
        "correct_first_batch_target = torch.LongTensor(\n",
        "    [[ 3396, 10303,   125, 13239,     0,     0,     0,     0,     0],\n",
        "     [ 1660,  5971,   785,   125,  1847, 13779, 15616,     0,     0]])\n",
        "\n",
        "print(first_batch_input)\n",
        "print(first_batch_target)\n",
        "\n",
        "# assert torch.equal(first_batch_input, correct_first_batch_input)\n",
        "# assert torch.equal(first_batch_target, correct_first_batch_target)\n",
        "\n",
        "print('Passou no assert de dataset.')"
      ],
      "metadata": {
        "id": "8r7jBFFUeApe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284,
          "referenced_widgets": [
            "28c4996c608f41abb0fb829b934f25be",
            "fc7ad48df86341a49afec6012845cc74",
            "55eee8c47ffc4b18a17ae6901695d222",
            "d7929a6719bd4c2589aa38a3190d75c0",
            "dcba05f54c9c4d4b8a4abe92a7dcf7bd",
            "f322c67311434e4999c53b981f40ca6e",
            "3bdeef6fbcfd40a085ae4b7758a03a4b",
            "492ca6f0c3d545778dd9d8506370dfae",
            "d39f2e06e98047e394d3cf60938276c4",
            "825e14cb5abf4076ba6b8b7cc7c337d0",
            "6ba54458906c454a9a08f09181a89764",
            "298f051fb63f4c7c90825285c7ed9831",
            "e2d8b5da8c3d48d495bfa23b45f7d99a",
            "3993ae92f1fc40f7bb1c115db62d0d7e",
            "5aae4244e83344fdbe54339d2384abf0",
            "da906a3b87da4d868877f529de6de6ea",
            "de6b503848084cf9944689b2cfc8d530",
            "30407a36393e44c89a0c3b0d160b40e4",
            "392a6b47f8434ee3a579c79d271d343f",
            "550c7db29e64489fb4f12358268db584",
            "bb12cee440ac4ec387590e2c1d22b5f6",
            "b0a992b87af244809bf0068cb45c26e2",
            "c9358d659a3a438aac735d3ba30b26de",
            "be3767758d55484daccb476c32e0effd",
            "3ef8d909ad66476e8929ec101bc0052a",
            "75b6ebef958541aa84950171d325e8ea",
            "a6983efda783484dacb2db6d81f0740b",
            "2bac18e142cc4dd598b93b583c81ee90",
            "7a7e1ac52e284325bcb181306d18344b",
            "f9eac330714f47898af6ce47a2a38f76",
            "382f2a14658c49eb8ae9b6ac0b13c2f1",
            "4d959a3b07764c0792372268dfddb420",
            "3792284e58b24e55b44b211c679fe124",
            "cc29c3667bd447059c9dc4739ee64b9c",
            "27fea7f0ada5442bbfc956b69ad342f0",
            "6c6dd2760a4d4baab493f302d4e455cc",
            "3809c9a60a5a4be488069186acb7f045",
            "fce39bf847064ca685cd029aabd9308f",
            "c22932a02ad4465f89e5618e9f8ad1f4",
            "c7fb62bda8794fd9b7008eba755bdf09",
            "7f641a39f115404fad5a9d922cb1a33a",
            "27dd862566754a2d9eb1429f5028deb4",
            "820cfdc5a1a44c03b4c0ed70bc4fcbbb",
            "dc3a24f07f1240449efcc5e3b2a397f0"
          ]
        },
        "outputId": "c6296512-761e-4996-fcbf-c752a9b1dedc"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28c4996c608f41abb0fb829b934f25be"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "298f051fb63f4c7c90825285c7ed9831"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c9358d659a3a438aac735d3ba30b26de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cc29c3667bd447059c9dc4739ee64b9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Passou no assert de tamanho do dataset.\n",
            "tensor([[  101,   142,  1358,  1301, 12223,  1260,  1884, 26875,     0,     0],\n",
            "        [  101,  2896,  1161,  1301,  8419,   182, 10950,  1186,  1260,  1435],\n",
            "        [  101,  1197, 13473,     0,     0,     0,     0,     0,     0,     0]])\n",
            "tensor([[  142,  1358,  1301, 12223,  1260,  1884, 26875,     0,     0,     0],\n",
            "        [ 2896,  1161,  1301,  8419,   182, 10950,  1186,  1260,  1435,     0],\n",
            "        [ 1197, 13473,     0,     0,     0,     0,     0,     0,     0,     0]])\n",
            "Passou no assert de dataset.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LfrHHouleJ0"
      },
      "source": [
        "# Carregamento do dataset "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2vFWjsSkmop"
      },
      "source": [
        "Iremos usar uma pequena amostra do dataset BookCorpus para treinar e avaliar nosso modelo de linguagem."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "4luQpPPdQWNw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "49942f95-1029-447b-ebdd-e4c72f8c0f3b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./downloads\n",
        "!wget -nc --directory-prefix=downloads https://storage.googleapis.com/huggingface-nlp/datasets/bookcorpus/bookcorpus.tar.bz2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eaGXd-3zVNaX",
        "outputId": "9a8c092f-6930-4808-f0b9-1e8b8112d1a2"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-07-18 19:43:40--  https://storage.googleapis.com/huggingface-nlp/datasets/bookcorpus/bookcorpus.tar.bz2\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 64.233.189.128, 74.125.204.128, 64.233.187.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|64.233.189.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1179510242 (1.1G) [application/x-bzip2]\n",
            "Saving to: ‘downloads/bookcorpus.tar.bz2’\n",
            "\n",
            "bookcorpus.tar.bz2  100%[===================>]   1.10G  64.4MB/s    in 25s     \n",
            "\n",
            "2022-07-18 19:44:07 (45.7 MB/s) - ‘downloads/bookcorpus.tar.bz2’ saved [1179510242/1179510242]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./data\n",
        "!tar -xf downloads/bookcorpus.tar.bz2 -C data/"
      ],
      "metadata": {
        "id": "026Ddq2jXBiH"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!rm downloads/*.tar.bz2"
      ],
      "metadata": {
        "id": "52rK7vsIgho7"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load datasets\n",
        "max_seq_length = 12\n",
        "\n",
        "# data set pequeno\n",
        "train_examples = 400000\n",
        "valid_examples = 800\n",
        "test_examples = 800\n",
        "\n",
        "path_text = f'./data/books_large_p1.txt'\n",
        "texts = open(path_text).readlines()\n",
        "\n",
        "n_concat=10\n",
        "\n",
        "print(f'Read {len(texts)//n_concat} lines.')\n",
        "\n",
        "max_lines = train_examples + valid_examples + test_examples\n",
        "print(f'Truncating to {max_lines} lines.')\n",
        "max_lin = max_lines * n_concat\n",
        "texts = texts[:max_lin]  \n",
        "text_aux = ''\n",
        "text_all = []\n",
        "j=0\n",
        "for i,text in enumerate(texts):\n",
        "    text_aux = f'{text_aux}{text.strip()} '\n",
        "    if (i+1)%n_concat==0:\n",
        "        text_all.append(text_aux)\n",
        "        j=j+1\n",
        "        text_aux=''\n",
        "\n",
        "texts = text_all \n",
        "\n",
        "training_texts = texts[:-(valid_examples + test_examples)]\n",
        "valid_texts = texts[-(valid_examples + test_examples):-test_examples]\n",
        "test_texts = texts[-test_examples:]\n",
        "\n",
        "training_dataset = MyDataset(texts=training_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "valid_dataset = MyDataset(texts=valid_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)\n",
        "test_dataset = MyDataset(texts=test_texts, tokenizer=tokenizer, max_seq_length=max_seq_length)"
      ],
      "metadata": {
        "id": "gxa_4gmiA-wE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fdd19a-567a-4b66-b169-6e49cd38614c"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Read 4000000 lines.\n",
            "Truncating to 401600 lines.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "texts[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "id": "_PMVVk3Z4VnI",
        "outputId": "b5ee450a-1ecc-4cef-ebe4-24f871b2d13b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'the half-ling book one in the fall of igneeria series kaylee soderburg copyright 2013 kaylee soderburg all rights reserved . isbn : 1492913731 isbn-13 : 978-1492913733 for my family , who encouraged me to never stop fighting for my dreams chapter 1 summer vacations supposed to be fun , right ? i wish i had a better answer to that question . starlings , new york is not the place youd expect much to happen . its a small quiet town , the kind where everyone knows your name . its a place where your parents wouldnt even care if you stayed out late biking with your friends . only because everyone felt so safe , so comfy . they dont know the half of it . but i do . i know it all and starlings is not the place where you want to be after dark . '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'training examples: {len(training_dataset)}')\n",
        "print(f'valid examples: {len(valid_dataset)}')\n",
        "print(f'test examples: {len(test_dataset)}')"
      ],
      "metadata": {
        "id": "KCSGJ5m7py4c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "367334e0-0ad7-425e-902a-36315b06fc3c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "training examples: 5887656\n",
            "valid examples: 10626\n",
            "test examples: 14891\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiheadAttention(torch.nn.Module):\n",
        "    def __init__(self, dim, n_head):\n",
        "        \"\"\"\n",
        "        Implements the Multi head Attention\n",
        "\n",
        "        Args:\n",
        "            dim : Dimension of the embedding layer for each word in the context.\n",
        "            n_layers : number of self-attention layers.\n",
        "        \"\"\"\n",
        "        super(MultiheadAttention, self).__init__()\n",
        "        \n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "\n",
        "        # Dimension de cada head\n",
        "        self.dim_head = self.dim // self.n_head\n",
        "\n",
        "        # As matrixes W_k, W_q, W_v, W_e   \n",
        "        self.W_k = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_q = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_v = nn.Linear(self.dim, self.dim , bias=False)\n",
        "        self.W_e = nn.Linear(self.dim, self.dim , bias=False)\n",
        "\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, key, value, query, mask = None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x is a LongTensor of shape (batch_size, max_seq_length, dimension)  (B,L,D)\n",
        "            mask is Tensor of shape (batch_size, 1, max_seq_length,max_seq_length) (B,1,L,L)\n",
        "        Returns:\n",
        "            LongTensor of shape (batch_size, max_seq_length, dimension)\n",
        "        \"\"\"\n",
        "        batch_size = value.size(0)\n",
        "        max_seq_length = value.size(1)\n",
        "\n",
        "        # k, q, v tem dimensão B, L, H, D/H \n",
        "        k = self.W_k(key).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "        q = self.W_q(query).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "        v = self.W_v(value).reshape(batch_size, max_seq_length, self.n_head, self.dim_head)\n",
        "\n",
        "        # Transpor para B, H, L, D/H\n",
        "        k = k.transpose(1,2)\n",
        "        q = q.transpose(1,2)\n",
        "        v = v.transpose(1,2)\n",
        "\n",
        "        # atention\n",
        "        scores = torch.matmul(q, torch.transpose(k,-1,-2))   # B, H, L, L\n",
        "\n",
        "        # aplicar mascara\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(~mask, float(\"-1e16\"))\n",
        "\n",
        "        scores = scores/math.sqrt(self.dim_head)\n",
        "\n",
        "        # aplicar sofmax\n",
        "        probs = self.softmax(scores) # B, H, L, L\n",
        "\n",
        "        probs  = torch.matmul(probs, v) # B, H, L, D/H \n",
        "\n",
        "        probs = probs.transpose(1,2).contiguous() # B, L, H, D/H \n",
        "        probs = probs.reshape(batch_size,max_seq_length, self.dim) # B, L, D\n",
        "\n",
        "        return self.W_e(probs)\n",
        "\n"
      ],
      "metadata": {
        "id": "a31ZiZEUdP9R"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(torch.nn.Module):\n",
        "    def __init__(self, dim: int, n_head: int, expansion_factor: int):\n",
        "        \"\"\"\n",
        "        Implements Transformer Block \n",
        "        Args:\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_head (int): number of self-attention head\n",
        "        \"\"\"\n",
        "        super(TransformerBlock,self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        self.multi_head = MultiheadAttention(self.dim, self.n_head)\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        hidden_size = self.expansion_factor * self.dim\n",
        "        self.feed_forward = nn.Sequential(\n",
        "            nn.Linear(self.dim, hidden_size),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_size, self.dim)\n",
        "        )\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(self.dim)\n",
        "        self.dropout2 = nn.Dropout(0.1)\n",
        "        \n",
        "    def forward(self, key, value, query, mask=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x is a LongTensor of shape (batch_size, max_seq_length, dimension)  (B,L,D)\n",
        "            mask is Tensor of shape (batch_size, 1, max_seq_length,max_seq_length) (B,1,L,L)\n",
        "        Returns:\n",
        "            LongTensor of shape (batch_size, max_seq_length, dimension)\n",
        "        \"\"\"\n",
        "\n",
        "        attention = self.multi_head(key, value, query, mask=mask)\n",
        "        attention_residual = attention + query\n",
        "        norm1_out = self.dropout1(self.norm1(attention_residual))\n",
        "        \n",
        "        feed_fwd = self.feed_forward(norm1_out)\n",
        "        feed_fwd_residual = feed_fwd + norm1_out\n",
        "        norm2_out = self.dropout2(self.norm2(feed_fwd_residual))\n",
        "        \n",
        "        return norm2_out\n"
      ],
      "metadata": {
        "id": "FA0PkKhVwjjA"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderBlock(torch.nn.Module):\n",
        "    def __init__(self, dim: int, n_head: int, expansion_factor: int):\n",
        "\n",
        "        super(DecoderBlock, self).__init__()\n",
        "\n",
        "        self.dim = dim\n",
        "        self.n_head = n_head\n",
        "        self.expasion_factor = expansion_factor\n",
        "\n",
        "        self.attention = MultiheadAttention(self.dim, self.n_head)\n",
        "        self.norm1 = nn.LayerNorm(self.dim)\n",
        "        self.dropout1 = nn.Dropout(0.1)\n",
        "\n",
        "        self.transformer_block = TransformerBlock(self.dim, self.n_head, self.expasion_factor)\n",
        "    \n",
        "    def forward(self, enc_out, x, mask_self,mask_cross=None):\n",
        "\n",
        "        attention = self.attention(x, x, x, mask_self)\n",
        "        attention_residual = attention + x\n",
        "        out = self.dropout1(self.norm1(attention_residual))\n",
        "\n",
        "        if enc_out is None:\n",
        "            enc_out = out\n",
        "        else:\n",
        "            mask_cross = None\n",
        "        out = self.transformer_block(enc_out,enc_out,out,mask_cross)\n",
        "\n",
        "        return out"
      ],
      "metadata": {
        "id": "HCXmrfIQrYu4"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGaAjYDfWdd1"
      },
      "source": [
        "class TransformerDecoder(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size: int, max_seq_length: int, dim: int, n_layers: int, pad_token_id: int, n_head: int, expansion_factor: int):\n",
        "        \"\"\"\n",
        "        Implements the Self-attention, decoder-only.\"\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): Size of the input vocabulary.\n",
        "            max_seq_length (int): Size of the sequence to consider as context for prediction.\n",
        "            dim (int): Dimension of the embedding layer for each word in the context.\n",
        "            n_layers (int): number of self-attention layers.\n",
        "            pad_token_id (int): id of the pad token that will be ignored in the attention.\n",
        "            n_head(int): number of head of self-attention\n",
        "            expansion_factor(int): fator for the hidden size of feed_forward\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        super(TransformerDecoder,self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.max_seq_length = max_seq_length\n",
        "        self.dim = dim\n",
        "        self.n_layers = n_layers\n",
        "        self.pad_token_id = pad_token_id\n",
        "\n",
        "        self.n_head = n_head\n",
        "        self.expansion_factor = expansion_factor\n",
        "\n",
        "        # C()\n",
        "        self.C_w = nn.Embedding(vocab_size, dim)\n",
        "\n",
        "        # P()\n",
        "        self.P_w = nn.Embedding(max_seq_length, dim)\n",
        "\n",
        "        self.dropout = nn.Dropout(0.1)\n",
        "\n",
        "        self.layers = nn.ModuleList([DecoderBlock(self.dim,self.n_head,self.expansion_factor) for i in range(self.n_layers)])\n",
        "\n",
        "        self.linear_out = nn.Linear(self.dim,self.vocab_size)\n",
        "\n",
        "        \n",
        "    def forward(self, inputs, enc_out=None):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            inputs is a LongTensor of shape (batch_size, max_seq_length)\n",
        "            \n",
        "        Returns:\n",
        "            logits of shape (batch_size, max_seq_length, vocab_size)\n",
        "        \"\"\"\n",
        "        # Escreva seu código aqui.\n",
        "        \n",
        "        batch_size = inputs.size(0)\n",
        "        max_seq_length = inputs.size(1)\n",
        "\n",
        "        c_emb = self.C_w(inputs)  # B,L,D\n",
        "        p_emb = self.P_w(torch.LongTensor(range(0,self.max_seq_length)).unsqueeze(0).to(inputs.device))\n",
        "\n",
        "\n",
        "        x = c_emb + p_emb  # B,L,D\n",
        "        x = self.dropout(x)\n",
        "\n",
        "        # generar mascara\n",
        "        mask_tri = (torch.ones(self.max_seq_length,self.max_seq_length).tril() == 1).expand(batch_size,1,self.max_seq_length,self.max_seq_length).to(inputs.device)\n",
        "        \n",
        "        mask_pad = (inputs != self.pad_token_id)\n",
        "        mask_pad = mask_pad.reshape(batch_size,1,1,self.max_seq_length).expand(batch_size,1,self.max_seq_length,self.max_seq_length).to(inputs.device)\n",
        "\n",
        "        mask = torch.logical_and(mask_tri,mask_pad)\n",
        "        mask_cross = mask_tri\n",
        "        for layer in self.layers:\n",
        "\n",
        "            x = layer(enc_out,x,mask_self=mask,mask_cross=mask_cross)   # B, L, D\n",
        "\n",
        "        out = self.linear_out(x) # B, L, vocab\n",
        "\n",
        "        return out"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste o modelo com um exemplo"
      ],
      "metadata": {
        "id": "Rm6_PTH2i98e"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RwnxfZlrZoT_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a35b580d-ac9b-40ce-8876-e9298adbad64"
      },
      "source": [
        "model = TransformerDecoder(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=256,\n",
        "    n_layers=4,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    n_head = 4,\n",
        "    expansion_factor = 4,\n",
        ").to(device)\n",
        "\n",
        "sample_input, _ = next(iter(DataLoader(training_dataset)))\n",
        "sample_input = sample_input.to(device)\n",
        "sample_output = model(sample_input)\n",
        "print(f'sample_input.shape: {sample_input.shape}')\n",
        "print(f'sample_output.shape: {sample_output.shape}')"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "sample_input.shape: torch.Size([1, 12])\n",
            "sample_output.shape: torch.Size([1, 12, 28996])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I3Vh6B-VkA01",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93c9c2e2-da76-4bc3-b0e4-9b5fa8bad150"
      },
      "source": [
        "num_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f'Number of model parameters: {num_params}')"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of model parameters: 19083588\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Assert da Perplexidade\n"
      ],
      "metadata": {
        "id": "8nhbUVsYnVAp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "random.seed(123)\n",
        "np.random.seed(123)\n",
        "torch.manual_seed(123)\n",
        "\n",
        "\n",
        "def perplexity(logits, target, ignore_token_id: int):\n",
        "    \"\"\"\n",
        "    Computes the perplexity.\n",
        "\n",
        "    Args:\n",
        "        logits: a FloatTensor of shape (batch_size, seq_length, vocab_size)\n",
        "        target: a LongTensor of shape (batch_size, seq_length)\n",
        "\n",
        "    Returns:\n",
        "        A float corresponding to the perplexity\n",
        "    \"\"\"\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target = target.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target, reduction='mean', ignore_index=ignore_token_id)\n",
        "    return torch.exp(loss)\n",
        "\n",
        "\n",
        "n_examples = 1000\n",
        "\n",
        "train_input_ids, train_target_ids = next(iter(DataLoader(training_dataset, batch_size=n_examples)))\n",
        "train_input_ids = train_input_ids.to(device)\n",
        "train_target_ids = train_target_ids.to(device)\n",
        "\n",
        "logits = model(train_input_ids)\n",
        "\n",
        "my_perplexity = perplexity(logits=logits, target=train_target_ids, ignore_token_id=tokenizer.pad_token_id)\n",
        "\n",
        "print(f'my perplexity:              {int(my_perplexity)}')\n",
        "print(f'correct initial perplexity: {tokenizer.vocab_size}')\n",
        "\n",
        "assert math.isclose(my_perplexity, tokenizer.vocab_size, abs_tol=7000)\n",
        "print('Passou o no assert da perplexidade')"
      ],
      "metadata": {
        "id": "gbMP8VAUncfX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d0f8137-f780-445c-8b91-61078bb5baf1"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "my perplexity:              34315\n",
            "correct initial perplexity: 28996\n",
            "Passou o no assert da perplexidade\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Laço de Treinamento e Validação"
      ],
      "metadata": {
        "id": "KiJtrsqPnE_l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_name = 'model_decoder_Bookcorpus.pt'\n",
        "path_modelo = F\"/content/drive/My Drive/modelo_proyecto/entrega_03/pre_trained/{model_save_name}\""
      ],
      "metadata": {
        "id": "lyb-He4AFHFP"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIMSaY-UUGUE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2378bf04-2b34-4a60-88bf-fa82eeba4252"
      },
      "source": [
        "max_examples = 150_000_000\n",
        "eval_every_steps = 1000\n",
        "lr = 3e-4\n",
        "\n",
        "\n",
        "dim = 256\n",
        "n_layers = 4\n",
        "n_head = 4\n",
        "expansion_factor = 4\n",
        "\n",
        "\n",
        "model = TransformerDecoder(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    max_seq_length=max_seq_length,\n",
        "    dim=dim,\n",
        "    n_layers=n_layers,\n",
        "    pad_token_id=tokenizer.pad_token_id,\n",
        "    n_head = n_head,\n",
        "    expansion_factor = expansion_factor,\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=64)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "checkpoint = {'vocab_size': tokenizer.vocab_size,\n",
        "              'max_seq_length': max_seq_length,\n",
        "              'dim': dim,\n",
        "              'n_layers': n_layers,\n",
        "              'pad_token_id': tokenizer.pad_token_id,\n",
        "              'n_head': n_head,\n",
        "              'expansion_factor': expansion_factor,\n",
        "              'model_state_dict': model.state_dict(),\n",
        "              'optimizer_state_dict' : optimizer.state_dict(),\n",
        "              'step': [],\n",
        "              'n_examples': [],\n",
        "              'train_ppl': [],\n",
        "              'valid_ppl': []\n",
        "}\n",
        "\n",
        "torch.save(checkpoint, path_modelo)\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "n_examples = 0\n",
        "step = 0\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_loss = np.average(train_losses)\n",
        "            train_ppl = np.exp(train_loss)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.6f}, train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "            \n",
        "            checkpoint['step'].append(step)\n",
        "            checkpoint['n_examples'].append(n_examples)\n",
        "            checkpoint['train_ppl'].append(train_ppl)\n",
        "            checkpoint['valid_ppl'].append(valid_ppl)\n",
        "\n",
        "            checkpoint['model_state_dict'] = model.state_dict()\n",
        "            checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "            torch.save(checkpoint, path_modelo)\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train loss: 10.437556, train ppl: 34117.18, valid ppl: 24231.57\n",
            "1000 steps; 64000 examples so far; train loss: 6.836864, train ppl: 931.56, valid ppl: 766.26\n",
            "2000 steps; 128000 examples so far; train loss: 6.358612, train ppl: 577.44, valid ppl: 421.02\n",
            "3000 steps; 192000 examples so far; train loss: 5.967823, train ppl: 390.65, valid ppl: 349.89\n",
            "4000 steps; 256000 examples so far; train loss: 5.802647, train ppl: 331.18, valid ppl: 318.71\n",
            "5000 steps; 320000 examples so far; train loss: 5.719031, train ppl: 304.61, valid ppl: 301.08\n",
            "6000 steps; 384000 examples so far; train loss: 5.652295, train ppl: 284.94, valid ppl: 284.15\n",
            "7000 steps; 448000 examples so far; train loss: 5.585078, train ppl: 266.42, valid ppl: 271.39\n",
            "8000 steps; 512000 examples so far; train loss: 5.531622, train ppl: 252.55, valid ppl: 264.44\n",
            "9000 steps; 576000 examples so far; train loss: 5.502714, train ppl: 245.36, valid ppl: 254.64\n",
            "10000 steps; 640000 examples so far; train loss: 5.463556, train ppl: 235.93, valid ppl: 248.29\n",
            "11000 steps; 704000 examples so far; train loss: 5.437132, train ppl: 229.78, valid ppl: 245.02\n",
            "12000 steps; 768000 examples so far; train loss: 5.411445, train ppl: 223.95, valid ppl: 240.36\n",
            "13000 steps; 832000 examples so far; train loss: 5.393254, train ppl: 219.92, valid ppl: 238.20\n",
            "14000 steps; 896000 examples so far; train loss: 5.365141, train ppl: 213.82, valid ppl: 232.49\n",
            "15000 steps; 960000 examples so far; train loss: 5.344677, train ppl: 209.49, valid ppl: 230.21\n",
            "16000 steps; 1024000 examples so far; train loss: 5.328722, train ppl: 206.17, valid ppl: 227.54\n",
            "17000 steps; 1088000 examples so far; train loss: 5.303518, train ppl: 201.04, valid ppl: 225.27\n",
            "18000 steps; 1152000 examples so far; train loss: 5.293382, train ppl: 199.02, valid ppl: 224.26\n",
            "19000 steps; 1216000 examples so far; train loss: 5.279489, train ppl: 196.27, valid ppl: 222.49\n",
            "20000 steps; 1280000 examples so far; train loss: 5.260210, train ppl: 192.52, valid ppl: 219.52\n",
            "21000 steps; 1344000 examples so far; train loss: 5.253865, train ppl: 191.30, valid ppl: 218.80\n",
            "22000 steps; 1408000 examples so far; train loss: 5.236339, train ppl: 187.98, valid ppl: 215.42\n",
            "23000 steps; 1472000 examples so far; train loss: 5.228970, train ppl: 186.60, valid ppl: 214.31\n",
            "24000 steps; 1536000 examples so far; train loss: 5.214534, train ppl: 183.93, valid ppl: 212.84\n",
            "25000 steps; 1600000 examples so far; train loss: 5.215007, train ppl: 184.01, valid ppl: 213.73\n",
            "26000 steps; 1664000 examples so far; train loss: 5.199743, train ppl: 181.23, valid ppl: 211.57\n",
            "27000 steps; 1728000 examples so far; train loss: 5.185481, train ppl: 178.66, valid ppl: 210.58\n",
            "28000 steps; 1792000 examples so far; train loss: 5.179494, train ppl: 177.59, valid ppl: 210.42\n",
            "29000 steps; 1856000 examples so far; train loss: 5.178203, train ppl: 177.36, valid ppl: 203.71\n",
            "30000 steps; 1920000 examples so far; train loss: 5.158545, train ppl: 173.91, valid ppl: 205.59\n",
            "31000 steps; 1984000 examples so far; train loss: 5.151453, train ppl: 172.68, valid ppl: 206.42\n",
            "32000 steps; 2048000 examples so far; train loss: 5.140331, train ppl: 170.77, valid ppl: 205.05\n",
            "33000 steps; 2112000 examples so far; train loss: 5.143810, train ppl: 171.37, valid ppl: 203.09\n",
            "34000 steps; 2176000 examples so far; train loss: 5.136568, train ppl: 170.13, valid ppl: 202.08\n",
            "35000 steps; 2240000 examples so far; train loss: 5.130092, train ppl: 169.03, valid ppl: 202.57\n",
            "36000 steps; 2304000 examples so far; train loss: 5.116811, train ppl: 166.80, valid ppl: 201.45\n",
            "37000 steps; 2368000 examples so far; train loss: 5.112137, train ppl: 166.02, valid ppl: 200.03\n",
            "38000 steps; 2432000 examples so far; train loss: 5.105662, train ppl: 164.95, valid ppl: 200.19\n",
            "39000 steps; 2496000 examples so far; train loss: 5.104620, train ppl: 164.78, valid ppl: 197.69\n",
            "40000 steps; 2560000 examples so far; train loss: 5.096254, train ppl: 163.41, valid ppl: 196.97\n",
            "41000 steps; 2624000 examples so far; train loss: 5.100082, train ppl: 164.04, valid ppl: 196.91\n",
            "42000 steps; 2688000 examples so far; train loss: 5.094569, train ppl: 163.13, valid ppl: 196.90\n",
            "43000 steps; 2752000 examples so far; train loss: 5.088188, train ppl: 162.10, valid ppl: 196.03\n",
            "44000 steps; 2816000 examples so far; train loss: 5.081948, train ppl: 161.09, valid ppl: 194.76\n",
            "45000 steps; 2880000 examples so far; train loss: 5.074109, train ppl: 159.83, valid ppl: 192.82\n",
            "46000 steps; 2944000 examples so far; train loss: 5.075355, train ppl: 160.03, valid ppl: 193.15\n",
            "47000 steps; 3008000 examples so far; train loss: 5.068257, train ppl: 158.90, valid ppl: 192.56\n",
            "48000 steps; 3072000 examples so far; train loss: 5.054457, train ppl: 156.72, valid ppl: 193.86\n",
            "49000 steps; 3136000 examples so far; train loss: 5.055495, train ppl: 156.88, valid ppl: 191.98\n",
            "50000 steps; 3200000 examples so far; train loss: 5.050139, train ppl: 156.04, valid ppl: 191.13\n",
            "51000 steps; 3264000 examples so far; train loss: 5.057698, train ppl: 157.23, valid ppl: 190.45\n",
            "52000 steps; 3328000 examples so far; train loss: 5.043665, train ppl: 155.04, valid ppl: 190.04\n",
            "53000 steps; 3392000 examples so far; train loss: 5.045591, train ppl: 155.34, valid ppl: 190.55\n",
            "54000 steps; 3456000 examples so far; train loss: 5.040590, train ppl: 154.56, valid ppl: 190.63\n",
            "55000 steps; 3520000 examples so far; train loss: 5.032846, train ppl: 153.37, valid ppl: 188.93\n",
            "56000 steps; 3584000 examples so far; train loss: 5.030722, train ppl: 153.04, valid ppl: 188.83\n",
            "57000 steps; 3648000 examples so far; train loss: 5.021587, train ppl: 151.65, valid ppl: 186.96\n",
            "58000 steps; 3712000 examples so far; train loss: 5.018569, train ppl: 151.19, valid ppl: 187.83\n",
            "59000 steps; 3776000 examples so far; train loss: 5.018182, train ppl: 151.14, valid ppl: 188.90\n",
            "60000 steps; 3840000 examples so far; train loss: 5.021540, train ppl: 151.64, valid ppl: 188.58\n",
            "61000 steps; 3904000 examples so far; train loss: 5.018180, train ppl: 151.14, valid ppl: 186.34\n",
            "62000 steps; 3968000 examples so far; train loss: 5.009830, train ppl: 149.88, valid ppl: 185.41\n",
            "63000 steps; 4032000 examples so far; train loss: 5.007464, train ppl: 149.53, valid ppl: 187.69\n",
            "64000 steps; 4096000 examples so far; train loss: 5.000669, train ppl: 148.51, valid ppl: 186.84\n",
            "65000 steps; 4160000 examples so far; train loss: 5.003844, train ppl: 148.98, valid ppl: 186.34\n",
            "66000 steps; 4224000 examples so far; train loss: 5.000885, train ppl: 148.54, valid ppl: 185.67\n",
            "67000 steps; 4288000 examples so far; train loss: 4.999292, train ppl: 148.31, valid ppl: 184.23\n",
            "68000 steps; 4352000 examples so far; train loss: 4.985286, train ppl: 146.25, valid ppl: 182.93\n",
            "69000 steps; 4416000 examples so far; train loss: 4.983948, train ppl: 146.05, valid ppl: 182.77\n",
            "70000 steps; 4480000 examples so far; train loss: 4.989882, train ppl: 146.92, valid ppl: 183.75\n",
            "71000 steps; 4544000 examples so far; train loss: 4.986325, train ppl: 146.40, valid ppl: 183.59\n",
            "72000 steps; 4608000 examples so far; train loss: 4.992138, train ppl: 147.25, valid ppl: 182.34\n",
            "73000 steps; 4672000 examples so far; train loss: 4.981751, train ppl: 145.73, valid ppl: 182.52\n",
            "74000 steps; 4736000 examples so far; train loss: 4.978585, train ppl: 145.27, valid ppl: 183.98\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-54017fc6512a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mn_examples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-54017fc6512a>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(input_ids, target_ids)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mtarget_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-21-024fc0a24f88>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, enc_out)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_self\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_cross\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_cross\u001b[0m\u001b[0;34m)\u001b[0m   \u001b[0;31m# B, L, D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_out\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# B, L, vocab\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-20-0ae3ab0d353c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, enc_out, x, mask_self, mask_cross)\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m             \u001b[0mmask_cross\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformer_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menc_out\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmask_cross\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-18-d0c49fc30e35>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, key, value, query, mask)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mfeed_fwd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm1_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mfeed_fwd_residual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeed_fwd\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnorm1_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mnorm2_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeed_fwd_residual\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnorm2_out\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    188\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    189\u001b[0m         return F.layer_norm(\n\u001b[0;32m--> 190\u001b[0;31m             input, self.normalized_shape, self.weight, self.bias, self.eps)\n\u001b[0m\u001b[1;32m    191\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2501\u001b[0m             \u001b[0mlayer_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2502\u001b[0m         )\n\u001b[0;32m-> 2503\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2504\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2505\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bc198a10-a50b-44a9-eefe-b8493013b62d",
        "id": "pGLLmAWybn9K"
      },
      "source": [
        "# seguir entrenando modelo guardado\n",
        "max_examples = 150_000_000\n",
        "eval_every_steps = 5000\n",
        "lr = 2.5e-4\n",
        "\n",
        "checkpoint = torch.load(path_modelo)\n",
        "model = TransformerDecoder(\n",
        "    vocab_size = checkpoint['vocab_size'],\n",
        "    max_seq_length = checkpoint['max_seq_length'],\n",
        "    dim = checkpoint['dim'],\n",
        "    n_layers = checkpoint['n_layers'],\n",
        "    pad_token_id = checkpoint['pad_token_id'],\n",
        "    n_head = checkpoint['n_head'],\n",
        "    expansion_factor = checkpoint['expansion_factor'],\n",
        ").to(device)\n",
        "\n",
        "train_loader = DataLoader(training_dataset, batch_size=64, shuffle=True, drop_last=True)\n",
        "validation_loader = DataLoader(valid_dataset, batch_size=64)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "\n",
        "def train_step(input_ids, target_ids):\n",
        "    model.train()\n",
        "    model.zero_grad()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "def validation_step(input_ids, target_ids):\n",
        "    model.eval()\n",
        "    logits = model(input_ids)\n",
        "    logits = logits.reshape(-1, logits.shape[-1])\n",
        "    target_ids = target_ids.reshape(-1)\n",
        "    loss = nn.functional.cross_entropy(logits, target_ids, ignore_index=model.pad_token_id)\n",
        "    return loss.item()\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "\n",
        "for s in zip(checkpoint['step'], checkpoint['n_examples'], checkpoint['train_ppl'], checkpoint['valid_ppl']):\n",
        "    print(f'{s[0]} steps; {s[1]} examples so far; train ppl: {s[2]:.2f}, valid ppl: {s[3]:.2f}')\n",
        "\n",
        "n_examples = checkpoint['n_examples'][-1]\n",
        "step = checkpoint['step'][-1]\n",
        "\n",
        "while n_examples < max_examples:\n",
        "    for train_input_ids, train_target_ids in train_loader:\n",
        "        loss = train_step(train_input_ids.to(device), train_target_ids.to(device)) \n",
        "        train_losses.append(loss)\n",
        "        \n",
        "        if step % eval_every_steps == 0:\n",
        "            train_loss = np.average(train_losses)\n",
        "            train_ppl = np.exp(train_loss)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                valid_ppl = np.exp(np.average([\n",
        "                    validation_step(val_input_ids.to(device), val_target_ids.to(device))\n",
        "                    for val_input_ids, val_target_ids in validation_loader]))\n",
        "\n",
        "            print(f'{step} steps; {n_examples} examples so far; train loss: {train_loss:.6f}, train ppl: {train_ppl:.2f}, valid ppl: {valid_ppl:.2f}')\n",
        "            train_losses = []\n",
        "            \n",
        "            checkpoint['step'].append(step)\n",
        "            checkpoint['n_examples'].append(n_examples)\n",
        "            checkpoint['train_ppl'].append(train_ppl)\n",
        "            checkpoint['valid_ppl'].append(valid_ppl)\n",
        "\n",
        "            checkpoint['model_state_dict'] = model.state_dict()\n",
        "            checkpoint['optimizer_state_dict'] = optimizer.state_dict()\n",
        "\n",
        "            torch.save(checkpoint, path_modelo)\n",
        "\n",
        "        n_examples += len(train_input_ids)  # Increment of batch size\n",
        "        step += 1\n",
        "        if n_examples >= max_examples:\n",
        "            break"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 steps; 0 examples so far; train ppl: 34117.18, valid ppl: 24231.57\n",
            "64000 steps; 64000 examples so far; train ppl: 931.56, valid ppl: 766.26\n",
            "128000 steps; 128000 examples so far; train ppl: 577.44, valid ppl: 421.02\n",
            "192000 steps; 192000 examples so far; train ppl: 390.65, valid ppl: 349.89\n",
            "256000 steps; 256000 examples so far; train ppl: 331.18, valid ppl: 318.71\n",
            "320000 steps; 320000 examples so far; train ppl: 304.61, valid ppl: 301.08\n",
            "384000 steps; 384000 examples so far; train ppl: 284.94, valid ppl: 284.15\n",
            "448000 steps; 448000 examples so far; train ppl: 266.42, valid ppl: 271.39\n",
            "512000 steps; 512000 examples so far; train ppl: 252.55, valid ppl: 264.44\n",
            "576000 steps; 576000 examples so far; train ppl: 245.36, valid ppl: 254.64\n",
            "640000 steps; 640000 examples so far; train ppl: 235.93, valid ppl: 248.29\n",
            "704000 steps; 704000 examples so far; train ppl: 229.78, valid ppl: 245.02\n",
            "768000 steps; 768000 examples so far; train ppl: 223.95, valid ppl: 240.36\n",
            "832000 steps; 832000 examples so far; train ppl: 219.92, valid ppl: 238.20\n",
            "896000 steps; 896000 examples so far; train ppl: 213.82, valid ppl: 232.49\n",
            "960000 steps; 960000 examples so far; train ppl: 209.49, valid ppl: 230.21\n",
            "1024000 steps; 1024000 examples so far; train ppl: 206.17, valid ppl: 227.54\n",
            "1088000 steps; 1088000 examples so far; train ppl: 201.04, valid ppl: 225.27\n",
            "1152000 steps; 1152000 examples so far; train ppl: 199.02, valid ppl: 224.26\n",
            "1216000 steps; 1216000 examples so far; train ppl: 196.27, valid ppl: 222.49\n",
            "1280000 steps; 1280000 examples so far; train ppl: 192.52, valid ppl: 219.52\n",
            "1344000 steps; 1344000 examples so far; train ppl: 191.30, valid ppl: 218.80\n",
            "1408000 steps; 1408000 examples so far; train ppl: 187.98, valid ppl: 215.42\n",
            "1472000 steps; 1472000 examples so far; train ppl: 186.60, valid ppl: 214.31\n",
            "1536000 steps; 1536000 examples so far; train ppl: 183.93, valid ppl: 212.84\n",
            "1600000 steps; 1600000 examples so far; train ppl: 184.01, valid ppl: 213.73\n",
            "1664000 steps; 1664000 examples so far; train ppl: 181.23, valid ppl: 211.57\n",
            "1728000 steps; 1728000 examples so far; train ppl: 178.66, valid ppl: 210.58\n",
            "1792000 steps; 1792000 examples so far; train ppl: 177.59, valid ppl: 210.42\n",
            "1856000 steps; 1856000 examples so far; train ppl: 177.36, valid ppl: 203.71\n",
            "1920000 steps; 1920000 examples so far; train ppl: 173.91, valid ppl: 205.59\n",
            "1984000 steps; 1984000 examples so far; train ppl: 172.68, valid ppl: 206.42\n",
            "2048000 steps; 2048000 examples so far; train ppl: 170.77, valid ppl: 205.05\n",
            "2112000 steps; 2112000 examples so far; train ppl: 171.37, valid ppl: 203.09\n",
            "2176000 steps; 2176000 examples so far; train ppl: 170.13, valid ppl: 202.08\n",
            "2240000 steps; 2240000 examples so far; train ppl: 169.03, valid ppl: 202.57\n",
            "2304000 steps; 2304000 examples so far; train ppl: 166.80, valid ppl: 201.45\n",
            "2368000 steps; 2368000 examples so far; train ppl: 166.02, valid ppl: 200.03\n",
            "2432000 steps; 2432000 examples so far; train ppl: 164.95, valid ppl: 200.19\n",
            "2496000 steps; 2496000 examples so far; train ppl: 164.78, valid ppl: 197.69\n",
            "2560000 steps; 2560000 examples so far; train ppl: 163.41, valid ppl: 196.97\n",
            "2624000 steps; 2624000 examples so far; train ppl: 164.04, valid ppl: 196.91\n",
            "2688000 steps; 2688000 examples so far; train ppl: 163.13, valid ppl: 196.90\n",
            "2752000 steps; 2752000 examples so far; train ppl: 162.10, valid ppl: 196.03\n",
            "2816000 steps; 2816000 examples so far; train ppl: 161.09, valid ppl: 194.76\n",
            "2880000 steps; 2880000 examples so far; train ppl: 159.83, valid ppl: 192.82\n",
            "2944000 steps; 2944000 examples so far; train ppl: 160.03, valid ppl: 193.15\n",
            "3008000 steps; 3008000 examples so far; train ppl: 158.90, valid ppl: 192.56\n",
            "3072000 steps; 3072000 examples so far; train ppl: 156.72, valid ppl: 193.86\n",
            "3136000 steps; 3136000 examples so far; train ppl: 156.88, valid ppl: 191.98\n",
            "3200000 steps; 3200000 examples so far; train ppl: 156.04, valid ppl: 191.13\n",
            "3264000 steps; 3264000 examples so far; train ppl: 157.23, valid ppl: 190.45\n",
            "3328000 steps; 3328000 examples so far; train ppl: 155.04, valid ppl: 190.04\n",
            "3392000 steps; 3392000 examples so far; train ppl: 155.34, valid ppl: 190.55\n",
            "3456000 steps; 3456000 examples so far; train ppl: 154.56, valid ppl: 190.63\n",
            "3520000 steps; 3520000 examples so far; train ppl: 153.37, valid ppl: 188.93\n",
            "3584000 steps; 3584000 examples so far; train ppl: 153.04, valid ppl: 188.83\n",
            "3648000 steps; 3648000 examples so far; train ppl: 151.65, valid ppl: 186.96\n",
            "3712000 steps; 3712000 examples so far; train ppl: 151.19, valid ppl: 187.83\n",
            "3776000 steps; 3776000 examples so far; train ppl: 151.14, valid ppl: 188.90\n",
            "3840000 steps; 3840000 examples so far; train ppl: 151.64, valid ppl: 188.58\n",
            "3904000 steps; 3904000 examples so far; train ppl: 151.14, valid ppl: 186.34\n",
            "3968000 steps; 3968000 examples so far; train ppl: 149.88, valid ppl: 185.41\n",
            "4032000 steps; 4032000 examples so far; train ppl: 149.53, valid ppl: 187.69\n",
            "4096000 steps; 4096000 examples so far; train ppl: 148.51, valid ppl: 186.84\n",
            "4160000 steps; 4160000 examples so far; train ppl: 148.98, valid ppl: 186.34\n",
            "4224000 steps; 4224000 examples so far; train ppl: 148.54, valid ppl: 185.67\n",
            "4288000 steps; 4288000 examples so far; train ppl: 148.31, valid ppl: 184.23\n",
            "4352000 steps; 4352000 examples so far; train ppl: 146.25, valid ppl: 182.93\n",
            "4416000 steps; 4416000 examples so far; train ppl: 146.05, valid ppl: 182.77\n",
            "4480000 steps; 4480000 examples so far; train ppl: 146.92, valid ppl: 183.75\n",
            "4544000 steps; 4544000 examples so far; train ppl: 146.40, valid ppl: 183.59\n",
            "4608000 steps; 4608000 examples so far; train ppl: 147.25, valid ppl: 182.34\n",
            "4672000 steps; 4672000 examples so far; train ppl: 145.73, valid ppl: 182.52\n",
            "4736000 steps; 4736000 examples so far; train ppl: 145.27, valid ppl: 183.98\n",
            "4740000 steps; 4992000 examples so far; train loss: 4.964949, train ppl: 143.30, valid ppl: 180.42\n",
            "4745000 steps; 5312000 examples so far; train loss: 4.953536, train ppl: 141.68, valid ppl: 179.27\n",
            "4750000 steps; 5632000 examples so far; train loss: 4.946385, train ppl: 140.67, valid ppl: 176.91\n",
            "4755000 steps; 5952000 examples so far; train loss: 4.936877, train ppl: 139.33, valid ppl: 177.63\n",
            "4760000 steps; 6272000 examples so far; train loss: 4.930177, train ppl: 138.40, valid ppl: 177.41\n",
            "4765000 steps; 6592000 examples so far; train loss: 4.922628, train ppl: 137.36, valid ppl: 176.05\n",
            "4770000 steps; 6912000 examples so far; train loss: 4.914761, train ppl: 136.29, valid ppl: 175.25\n",
            "4775000 steps; 7232000 examples so far; train loss: 4.906985, train ppl: 135.23, valid ppl: 174.12\n",
            "4780000 steps; 7552000 examples so far; train loss: 4.897487, train ppl: 133.95, valid ppl: 172.65\n",
            "4785000 steps; 7872000 examples so far; train loss: 4.895160, train ppl: 133.64, valid ppl: 171.09\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-c5b24068cfaa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0mn_examples\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mmax_examples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m         \u001b[0mtrain_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-32-c5b24068cfaa>\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(input_ids, target_ids)\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mtarget_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctional\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_entropy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    394\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 396\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avaliação final no dataset de teste\n",
        "\n",
        "\n",
        "Bonus: o modelo com menor perplexidade no dataset de testes ganhará 0.5 ponto na nota final."
      ],
      "metadata": {
        "id": "VgdNymJdNPXP"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nxN5YytzZ7Tn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6df084c4-5349-486b-91d9-6abcbc567ed6"
      },
      "source": [
        "test_loader = DataLoader(test_dataset, batch_size=64)\n",
        "\n",
        "with torch.no_grad():\n",
        "    test_ppl = np.exp(np.average([\n",
        "        validation_step(test_input_ids.to(device), test_target_ids.to(device))\n",
        "        for test_input_ids, test_target_ids in test_loader\n",
        "    ]))\n",
        "\n",
        "print(f'test perplexity: {test_ppl}')"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "test perplexity: 161.88555274217737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Teste seu modelo com uma sentença\n",
        "\n",
        "Escolha uma sentença gerada pelo modelo que ache interessante."
      ],
      "metadata": {
        "id": "BHvEs8mPszy_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'A man is eating pizza in the park at night with a'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "id": "-CFElf4tsytW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4f609e0-7da9-4086-b31e-7ea803103ea1"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A man is eating pizza in the park at night with a copyright\n",
            "A man is eating pizza in the park at night with a copyright 2014\n",
            "A man is eating pizza in the park at night with a copyright 2014 :\n",
            "A man is eating pizza in the park at night with a copyright 2014 : /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www.\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / / / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / / / / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / / / / / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / / / / / / / / /\n",
            "A man is eating pizza in the park at night with a copyright 2014 : / / www. / / / / / / / / / / / / /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The young dog is sleeping in the park and playing with an'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae24e225-e74c-469b-9bfc-97ceef97efbb",
        "id": "EwHpxq6zltEC"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The young dog is sleeping in the park and playing with an hour\n",
            "The young dog is sleeping in the park and playing with an hour.\n",
            "The young dog is sleeping in the park and playing with an hour. to\n",
            "The young dog is sleeping in the park and playing with an hour. to the\n",
            "The young dog is sleeping in the park and playing with an hour. to the in\n",
            "The young dog is sleeping in the park and playing with an hour. to the india\n",
            "The young dog is sleeping in the park and playing with an hour. to the india,\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been a\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been a good\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been a good idea\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been a good idea of\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been a good idea of the\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been a good idea of the the\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been a good idea of the the author\n",
            "The young dog is sleeping in the park and playing with an hour. to the india, the united states had been a good idea of the the author.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = 'The old dog is running in the park and playing with a'\n",
        "max_output_tokens = 20\n",
        "model.eval()\n",
        "\n",
        "for _ in range(max_output_tokens):\n",
        "    input_ids = tokenize(text=prompt, tokenizer=tokenizer)\n",
        "    \n",
        "    input_ids_truncated = input_ids[-max_seq_length:]  # Usamos apenas os últimos <max_seq_length> tokens como entrada para o modelo.\n",
        "    logits = model(torch.LongTensor([input_ids_truncated]).to(device))\n",
        "    logits = logits[:, -1, :]  # Usamos apenas o ultimo token da sequencia\n",
        "    # Ao usarmos o argmax, a saída do modelo em cada passo é o token de maior probabilidade.\n",
        "    # Isso se chama decodificação gulosa (greedy decoding).\n",
        "    predicted_id = torch.argmax(logits).item()\n",
        "    input_ids += [predicted_id]  # Concatenamos a entrada com o token escolhido nesse passo.\n",
        "    prompt = tokenizer.decode(input_ids)\n",
        "    print(prompt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aY8iKefm8IiU",
        "outputId": "42de626c-84f4-4aba-fad5-6bf828d9d58c"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The old dog is running in the park and playing with a new\n",
            "The old dog is running in the park and playing with a new yo\n",
            "The old dog is running in the park and playing with a new york\n",
            "The old dog is running in the park and playing with a new york.\n",
            "The old dog is running in the park and playing with a new york..\n",
            "The old dog is running in the park and playing with a new york.. south\n",
            "The old dog is running in the park and playing with a new york.. south,\n",
            "The old dog is running in the park and playing with a new york.. south, chapter\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 :\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http :\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http : /\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http : / /\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http : / / /\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http : / / / /\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http : / / / / /\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http : / / / / / /\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http : / / / / / / /\n",
            "The old dog is running in the park and playing with a new york.. south, chapter 2 : http : / / / / / / / /\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bonus 1\n",
        "Quem conseguir a menor perplexidade no dataset de testes ganha 0.5 ponto na média final.\n",
        "\n",
        "## Bonus 2\n",
        "Qual é a complexidade (em notação O-grande) da função de geração de texto acima?\n",
        "\n",
        "Quem responder corretamente a pergunta acima e deixar a função com menor complexidade ganha 0.5 ponto na média final."
      ],
      "metadata": {
        "id": "nGdxlXhGq7Ua"
      }
    }
  ]
}